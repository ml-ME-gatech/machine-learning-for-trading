{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and store STOOQ data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains information on downloading the STOOQ stock and ETF price data that we use in [Chapter 09](../09_time_series_models) for a pairs trading strategy based on cointegration and [Chapter 11](../11_decision_trees_random_forests) for a long-short strategy using Random Forest return predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T14:32:07.092623Z",
     "start_time": "2020-06-18T14:32:07.090885Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T14:32:07.263130Z",
     "start_time": "2020-06-18T14:32:07.259861Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile, BadZipFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Data Store path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the path to the `DATA_STORE` if you would like to store the data elsewhere and change the notebooks accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:27:54.832609Z",
     "start_time": "2020-06-19T02:27:54.824778Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_STORE = Path('assets.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stooq Historical Market Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the below downloading details may change at any time as Stooq updates their website; if you encounter errors, please inspect their website and raise a GitHub issue to let us know so we can update the information.\n",
    "\n",
    "> Update 12/2020: please note that STOOQ will disable automatic downloads and require CAPTCHA starting Dec 10, 2020 so that the code that downloads and unpacks the zip files will no longer work; please navigate to their website [here](https://stooq.com/db/h/) for manual download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download price data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download **price data** for the selected combination of asset class, market and frequency from [the Stooq website](https://stooq.com/db/h/)\n",
    "2. Store the result under `stooq` using the preferred folder structure outlined on the website. It has the structure: `/data/freq/market/asset_class`, such as `/data/daily/us/nasdaq etfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:17:23.456087Z",
     "start_time": "2020-06-19T02:17:23.449798Z"
    }
   },
   "outputs": [],
   "source": [
    "stooq_path = Path('stooq') \n",
    "if not stooq_path.exists():\n",
    "    stooq_path.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the symbol for the market you want to download price data for. In this book we'll be useing `us` and `jp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:17:26.841192Z",
     "start_time": "2020-06-19T02:17:26.835288Z"
    }
   },
   "outputs": [],
   "source": [
    "STOOQ_URL = 'https://static.stooq.com/db/h/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:23:15.379159Z",
     "start_time": "2020-06-19T02:23:01.883773Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_price_data(market='us',\n",
    "                        freq = 'd'):\n",
    "    data_name = stooq_path.joinpath('.downloaded',f'{freq}_{market}_txt.zip')\n",
    "    if not data_name.exists():\n",
    "        raise FileNotFoundError(f'You must download the data first. {data_name} not found.')\n",
    "    with ZipFile(data_name) as zip_file:\n",
    "        for i, file in enumerate(zip_file.namelist()):\n",
    "            if not file.endswith('.txt'):\n",
    "                continue\n",
    "            local_file = stooq_path / file\n",
    "            local_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            try:\n",
    "                with local_file.open('wb') as output:\n",
    "                    for line in zip_file.open(file).readlines():\n",
    "                        output.write(line)\n",
    "\n",
    "            except FileNotFoundError as fnfe:\n",
    "                print(fnfe)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:17:23.882490Z",
     "start_time": "2020-06-19T02:17:23.879665Z"
    }
   },
   "outputs": [],
   "source": [
    "for market in ['us', 'jp']:\n",
    "    download_price_data(market=market)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T20:43:47.976210Z",
     "start_time": "2020-04-18T20:43:47.968770Z"
    }
   },
   "source": [
    "Add the corresponding **symbols**, i.e., tickers and names by following the directory tree on the same site. You can also adapt the following code snippet using the appropriate asset code that you find by inspecting the url; this example works for NASDAQ ETFs that have code `g=69`:\n",
    "```python\n",
    "df = pd.read_csv('https://stooq.com/db/l/?g=69', sep='        ').apply(lambda x: x.str.strip())\n",
    "df.columns = ['ticker', 'name']\n",
    "df.drop_duplicates('ticker').to_csv('stooq/data/tickers/us/nasdaq etfs.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T01:30:28.894603Z",
     "start_time": "2020-06-19T01:30:28.885132Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata_dict = {\n",
    "    ('jp', 'tse etfs'): 34,\n",
    "    ('jp', 'tse stocks'): 32,\n",
    "    ('us', 'nasdaq etfs'): 69,\n",
    "    ('us', 'nasdaq stocks'): 27,\n",
    "    ('us', 'nyse etfs'): 70,\n",
    "    ('us', 'nyse stocks'): 28,\n",
    "    ('us', 'nysemkt stocks'): 26\n",
    "}\n",
    "\n",
    "metadata_dict = {\n",
    "    ('us', 'nasdaq etfs','d'): 70,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T01:36:26.300430Z",
     "start_time": "2020-06-19T01:36:22.212007Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (market, asset_class,freq), code \u001b[38;5;129;01min\u001b[39;00m metadata_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      3\u001b[0m     path \u001b[38;5;241m=\u001b[39m meta_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfreq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmarket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ms/data/daily/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmarket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00masset_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/EMASTER\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m      5\u001b[0m     df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop_duplicates(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[1;32mc:\\Users\\mlanahan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mlanahan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\mlanahan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mlanahan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mlanahan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "meta_path = Path('stooq') / 'data/meta'\n",
    "for (market, asset_class,freq), code in metadata_dict.items():\n",
    "    path = meta_path / f'{freq}_{market}_ms/data/daily/{market}/{asset_class}/EMASTER'\n",
    "    df = pd.read_csv(path, sep=r'\\s+').apply(lambda x: x.str.strip())\n",
    "    df.columns = ['ticker', 'name']\n",
    "    df = df.drop_duplicates('ticker').dropna()\n",
    "    print(market, asset_class, f'# tickers: {df.shape[0]:,.0f}')\n",
    "    path = stooq_path / 'tickers' / market\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)\n",
    "    df.to_csv(path / f'{asset_class}.csv', index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store price data in HDF5 format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up loading, we store the price data in HDF format. The function `get_stooq_prices_and_symbols` loads data assuming the directory structure described above and takes the following arguments:\n",
    "- frequency (see Stooq website for options as these may change; default is `daily`\n",
    "- market (default: `us`), and \n",
    "- asset class (default: `nasdaq etfs`.\n",
    "\n",
    "It removes files that do not have data or do not appear in the corresponding list of symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:28:05.958722Z",
     "start_time": "2020-06-19T02:28:05.940267Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_stooq_prices_and_tickers(frequency='daily',\n",
    "                                 market='us',\n",
    "                                 asset_class='nasdaq etfs'):\n",
    "    prices = []\n",
    "\n",
    "\n",
    "    if frequency in ['5 min', 'hourly']:\n",
    "        parse_dates = [['date', 'time']]\n",
    "        date_label = 'date_time'\n",
    "    else:\n",
    "        parse_dates = ['date']\n",
    "        date_label = 'date'\n",
    "    names = ['ticker', 'freq', 'date', 'time', \n",
    "             'open', 'high', 'low', 'close','volume', 'openint']\n",
    "    \n",
    "    usecols = ['ticker', 'open', 'high', 'low', 'close', 'volume'] + parse_dates\n",
    "    path = stooq_path / 'data/data' / frequency / market / asset_class\n",
    "    print(path.as_posix())\n",
    "    files = path.glob('**/*.txt')\n",
    "    tickers = {}\n",
    "    for i, file in enumerate(files, 1):\n",
    "        if i % 500 == 0:\n",
    "            print(i)\n",
    "        try:\n",
    "            df = (pd.read_csv(\n",
    "                file,\n",
    "                names=names,\n",
    "                usecols=usecols,\n",
    "                header=0,\n",
    "                parse_dates=parse_dates))\n",
    "            prices.append(df)\n",
    "            tickers[df.ticker.iloc[0]] = (file.stem.split('.')[0].lower())\n",
    "        except (pd.errors.EmptyDataError, IndexError):\n",
    "            print('\\tdata missing', file.stem)\n",
    "            #file.unlink()\n",
    "\n",
    "    prices = (pd.concat(prices, ignore_index=True)\n",
    "              .rename(columns=str.lower)\n",
    "              .set_index(['ticker', date_label])\n",
    "              .apply(lambda x: pd.to_numeric(x, errors='coerce')))\n",
    "    return prices,pd.Series(tickers,name = 'ticker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using US equities and ETFs in [Chapter 9](../09_time_series_models) and and Japanese equities in [Chapter 11](../11_decision_trees_random_forests). The following code collects the price data for the period 2000-2019 and stores it with the corresponding symbols in the global `assets.h5` store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:30:25.079270Z",
     "start_time": "2020-06-19T02:28:06.594886Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nasdaq etfs\n",
      "stooq/data/data/daily/us/nasdaq etfs\n",
      "\n",
      "No. of observations per asset\n",
      "count     301.000000\n",
      "mean     1732.063123\n",
      "std      1135.174568\n",
      "min         2.000000\n",
      "25%       804.000000\n",
      "50%      1677.000000\n",
      "75%      2542.000000\n",
      "max      5031.000000\n",
      "dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 521351 entries, ('AADR.US', Timestamp('2010-07-21 00:00:00')) to ('YLDE.US', Timestamp('2019-12-27 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   open    521351 non-null  float64\n",
      " 1   high    521351 non-null  float64\n",
      " 2   low     521351 non-null  float64\n",
      " 3   close   521351 non-null  float64\n",
      " 4   volume  521351 non-null  float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 22.1+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mlanahan\\AppData\\Local\\Temp\\ipykernel_14628\\2539056611.py:31: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  prices.to_hdf(DATA_STORE, key + 'prices', format='t')\n",
      "C:\\Users\\mlanahan\\AppData\\Local\\Temp\\ipykernel_14628\\2539056611.py:33: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  tickers.to_hdf(DATA_STORE, key + 'tickers', format='t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nasdaq stocks\n",
      "stooq/data/data/daily/us/nasdaq stocks\n",
      "500\n",
      "\tdata missing biya.us\n",
      "1000\n",
      "\tdata missing cubwu.us\n",
      "\tdata missing ehldv.us\n",
      "\tdata missing eseav.us\n",
      "1500\n",
      "\tdata missing fvnnu.us\n",
      "2000\n",
      "\tdata missing lifw.us\n",
      "2500\n",
      "3000\n",
      "\tdata missing pirs.us\n",
      "\tdata missing pqap.us\n",
      "3500\n",
      "4000\n",
      "\tdata missing wtf.us\n",
      "4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mlanahan\\AppData\\Local\\Temp\\ipykernel_14628\\3142032891.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  prices = (pd.concat(prices, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of observations per asset\n",
      "count    2063.000000\n",
      "mean     2273.589918\n",
      "std      1511.723111\n",
      "min         2.000000\n",
      "25%       740.000000\n",
      "50%      2355.000000\n",
      "75%      3737.000000\n",
      "max      5031.000000\n",
      "dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 4690416 entries, ('AACG.US', Timestamp('2008-01-28 00:00:00')) to ('ZYXI.US', Timestamp('2019-12-31 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   open    float64\n",
      " 1   high    float64\n",
      " 2   low     float64\n",
      " 3   close   float64\n",
      " 4   volume  float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 197.1+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mlanahan\\AppData\\Local\\Temp\\ipykernel_14628\\2539056611.py:31: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  prices.to_hdf(DATA_STORE, key + 'prices', format='t')\n",
      "C:\\Users\\mlanahan\\AppData\\Local\\Temp\\ipykernel_14628\\2539056611.py:33: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  tickers.to_hdf(DATA_STORE, key + 'tickers', format='t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nyse etfs\n",
      "stooq/data/data/daily/us/nyse etfs\n",
      "500\n",
      "\tdata missing dwcr.us\n",
      "\tdata missing esgb.us\n",
      "\tdata missing gast.us\n",
      "1000\n",
      "\tdata missing hyke.us\n",
      "1500\n",
      "2000\n",
      "\tdata missing rois.us\n",
      "2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mlanahan\\AppData\\Local\\Temp\\ipykernel_14628\\3142032891.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  prices = (pd.concat(prices, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of observations per asset\n",
      "count    1416.000000\n",
      "mean     1829.812147\n",
      "std      1268.127246\n",
      "min         1.000000\n",
      "25%       583.750000\n",
      "50%      1728.500000\n",
      "75%      3076.250000\n",
      "max      3738.000000\n",
      "dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 2591014 entries, ('AAAU.US', Timestamp('2018-08-15 00:00:00')) to ('ZSL.US', Timestamp('2019-12-31 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   open    float64\n",
      " 1   high    float64\n",
      " 2   low     float64\n",
      " 3   close   float64\n",
      " 4   volume  float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 108.9+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mlanahan\\AppData\\Local\\Temp\\ipykernel_14628\\2539056611.py:31: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  prices.to_hdf(DATA_STORE, key + 'prices', format='t')\n",
      "C:\\Users\\mlanahan\\AppData\\Local\\Temp\\ipykernel_14628\\2539056611.py:33: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  tickers.to_hdf(DATA_STORE, key + 'tickers', format='t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nyse stocks\n",
      "stooq/data/data/daily/us/nyse stocks\n",
      "\tdata missing apus.us\n",
      "500\n",
      "\tdata missing cpnm.us\n",
      "\tdata missing cpsr.us\n",
      "\tdata missing dabs.us\n",
      "1000\n",
      "\tdata missing ftki.us\n",
      "\tdata missing gumi.us\n",
      "\tdata missing hleo.us\n",
      "\tdata missing htax.us\n",
      "1500\n",
      "\tdata missing key.us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mlanahan\\AppData\\Local\\Temp\\ipykernel_14628\\3142032891.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  prices = (pd.concat(prices, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of observations per asset\n",
      "count    1088.000000\n",
      "mean     2720.434743\n",
      "std      1536.633498\n",
      "min         2.000000\n",
      "25%      1316.000000\n",
      "50%      3555.500000\n",
      "75%      3737.000000\n",
      "max      5031.000000\n",
      "dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 2959833 entries, ('A.US', Timestamp('2000-01-03 00:00:00')) to ('KEX.US', Timestamp('2019-12-31 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   open    float64\n",
      " 1   high    float64\n",
      " 2   low     float64\n",
      " 3   close   float64\n",
      " 4   volume  float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 124.4+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mlanahan\\AppData\\Local\\Temp\\ipykernel_14628\\2539056611.py:31: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  prices.to_hdf(DATA_STORE, key + 'prices', format='t')\n",
      "C:\\Users\\mlanahan\\AppData\\Local\\Temp\\ipykernel_14628\\2539056611.py:33: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  tickers.to_hdf(DATA_STORE, key + 'tickers', format='t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nysemkt stocks\n",
      "stooq/data/data/daily/us/nysemkt stocks\n",
      "\n",
      "No. of observations per asset\n",
      "count     195.000000\n",
      "mean     2700.430769\n",
      "std      1265.881604\n",
      "min         4.000000\n",
      "25%      1652.000000\n",
      "50%      3423.000000\n",
      "75%      3728.500000\n",
      "max      5029.000000\n",
      "dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 526584 entries, ('ACCS.US', Timestamp('2008-03-10 00:00:00')) to ('ZOMDF.US', Timestamp('2019-12-31 00:00:00'))\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   open    526584 non-null  float64\n",
      " 1   high    526584 non-null  float64\n",
      " 2   low     526584 non-null  float64\n",
      " 3   close   526584 non-null  float64\n",
      " 4   volume  526584 non-null  float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 22.3+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mlanahan\\AppData\\Local\\Temp\\ipykernel_14628\\2539056611.py:31: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  prices.to_hdf(DATA_STORE, key + 'prices', format='t')\n",
      "C:\\Users\\mlanahan\\AppData\\Local\\Temp\\ipykernel_14628\\2539056611.py:33: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  tickers.to_hdf(DATA_STORE, key + 'tickers', format='t')\n"
     ]
    }
   ],
   "source": [
    "# load some Japanese and all US assets for 2000-2019\n",
    "#markets = {'jp': ['tse stocks'],\n",
    "#           'us': ['nasdaq etfs', 'nasdaq stocks', 'nyse etfs', 'nyse stocks', 'nysemkt stocks']\n",
    "#          }\n",
    "markets = {'us': ['nasdaq etfs', 'nasdaq stocks', 'nyse etfs', 'nyse stocks', 'nysemkt stocks']}\n",
    "\n",
    "frequency = 'daily'\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "for market, asset_classes in markets.items():\n",
    "    for asset_class in asset_classes:\n",
    "        print(f'\\n{asset_class}')\n",
    "        prices,tickers = get_stooq_prices_and_tickers(frequency=frequency, \n",
    "                                                       market=market, \n",
    "                                                       asset_class=asset_class)\n",
    "        \n",
    "        prices = prices.sort_index().loc[idx[:, '2000': '2019'], :]\n",
    "        names = prices.index.names\n",
    "        prices = (prices\n",
    "                  .reset_index()\n",
    "                  .drop_duplicates()\n",
    "                  .set_index(names)\n",
    "                  .sort_index())\n",
    "        \n",
    "        print('\\nNo. of observations per asset')\n",
    "        print(prices.groupby('ticker').size().describe())\n",
    "        key = f'stooq/{market}/{asset_class.replace(\" \", \"/\")}/'\n",
    "        \n",
    "        print(prices.info())\n",
    "        \n",
    "        prices.to_hdf(DATA_STORE, key + 'prices', format='t')\n",
    "        \n",
    "        tickers.to_hdf(DATA_STORE, key + 'tickers', format='t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
